<!doctype html>
<html lang="en">
  
  <head>
    <meta charset="utf-8">
    
    <title>Keep it simple</title>
    
    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">
    
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    
    <link rel="stylesheet" href="bootstrap/css/bootstrap.min.css">
    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/sky.css" id="theme">
    <link rel="stylesheet" href="css/custom.css">
    
    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">
    
    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = 'css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>
    
    <!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
  </head>
  
  <body>
    
    <div class="reveal">
      <div class="slides">
        <section>
          <h2>Keep It Simple:
            <br>Text simplification using
            <br>Simple English Wikipedia
          </h2>
          <h3><a href="http://readability.crydee.eu/" target="_blank">http://readability.crydee.eu/</a></h3>
          <div>
            <strong class="text-primary">Internship defense</strong>
            — <strong class="text-primary">Hugo Mougard</strong>
            <br>
            <strong class="text-muted">July the 10ᵗʰ</strong>
          </div>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li>Definition: what is readability?</li>
              <li>Research statement</li>
              <li>Related works:
                <ul>
                  <li>Readability formulas</li>
                  <li>Machine learning</li>
                  <li>Simple Wikipedia</li>
                </ul>
                <li>Idea</li>
                <li>Corpus creation</li>
                <li>Phrase lexicon learning</li>
                <li>Fine-grained readability measure</li>
                <li>Text rewriting</li>
                <li>Future works</li>
                <li>Conclusion</li>
            </ul>
          </section>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li><strong class="text-danger">Definition: what is readability?</strong></li>
              <li>Research statement</li>
              <li>Related works:
                <ul>
                  <li>Readability formulas</li>
                  <li>Machine learning</li>
                  <li>Simple Wikipedia</li>
                </ul>
                <li>Idea</li>
                <li>Corpus creation</li>
                <li>Phrase lexicon learning</li>
                <li>Fine-grained readability measure</li>
                <li>Text rewriting</li>
                <li>Future works</li>
                <li>Conclusion</li>
            </ul>
          </section>
          <section>
            <h2>But first, what it is not</h2>
            <p>Readability is not legibility: the former is only about
              the text, not the layout nor the appearance.</p>
          </section>
          <section>
            <h2>Readability, a broad topic…</h2>
            <blockquote>
              <p>
                Readability is the <strong>sum of all the elements</strong>
                within a text that affect the success a group of
                readers have with it. The success is the extent to
                which they <strong>understand</strong> it, read it at
                <strong>optimal speed</strong>, and find it
                <strong>interesting</strong>
              </p>
              <footer>Edited from Dale and Chall, 1949</footer>
            </blockquote>
            <aside class="notes">
              most complete
            </aside>
          </section>
          <section>
            <h2>All the elements</h2>
            <h3>Depending on the reader</h3>
            <ul>
              <li>background knowledge</li>
              <li>reading fluency</li>
              <li>language</li>
            </ul>
          </section>
          <section>
            <h2>All the elements</h2>
            <h3>Depending on the text</h3>
            <ul>
              <li>syntax</li>
              <li>vocabulary</li>
              <li>idea density</li>
              <li>cognitive load</li>
            </ul>
          </section>
          <section>
            <h2>In this work</h2>
            <ul>
              <li class="text-warning">syntax</li>
              <li class="text-danger">vocabulary</li>
              <li>idea density</li>
              <li>cognitive load</li>
            </ul>
          </section>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li>Definition: what is readability?</li>
              <li><strong class="text-warning">Research statement</strong></li>
              <li>Related works:
                <ul>
                  <li>Readability formulas</li>
                  <li>Machine learning</li>
                  <li>Simple Wikipedia</li>
                </ul>
                <li>Idea</li>
                <li>Corpus creation</li>
                <li>Phrase lexicon learning</li>
                <li>Fine-grained readability measure</li>
                <li>Text rewriting</li>
                <li>Future works</li>
                <li>Conclusion</li>
            </ul>
          </section>
          <section>
            <h2><strong>Fine-grained</strong>
              <br>readability analysis</h2>
              <p>Most approaches consider the document as a <strong>whole</strong>.</p>
              <p>We want to be more specific.</p>
          </section>
          <section>
            <h2>End goal</h2>
              <p>Be able to find which words or sentences to rewrite, and how.</p>
          </section>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li>Definition: what is readability?</li>
              <li>Research statement</li>
              <li><strong class="text-primary">Related works:</strong>
                <ul>
                  <li><strong class="text-primary">Readability formulas</strong></li>
                  <li>Machine learning</li>
                  <li>Simple Wikipedia</li>
                </ul>
                <li>Idea</li>
                <li>Corpus creation</li>
                <li>Phrase lexicon learning</li>
                <li>Fine-grained readability measure</li>
                <li>Text rewriting</li>
                <li>Future works</li>
                <li>Conclusion</li>
            </ul>
          </section>
          <section>
            <h2>Early readability research</h2>
            <p>20ᵗʰ century research was centered around formulas to
              estimate if a text is readable ot not.</p>
          </section>
          <section>
            <h2>An efficient set of features</h2>
            <p>Most formulas use a combination of:</p>
            <ul>
              <li>average number of words per sentence</li>
              <li>average number of syllables per word</li>
              <li>presence of the word in a list of easy words</li>
            </ul>
          </section>
          <section>
            <h2>Flesch Reading Ease</h2>
            <p>
               (Flesch, 1948), ~100 = easy, <30 = somewhat hard for a
               good reader:
               <br>
               <br>
               $206.835
               - 1.015 \left(\frac{\text{total words}}{\text{total
               sentences}}\right)
               - 84.6 \left(\frac{\text{total syllables}}{\text{total words}}\right)$
            </p>
          </section>
          <section>
            <h2>Dale–Chall readability formula</h2>
            <p>
               (Dale and Chall, 1949):
               <br>
               <br>
               $0.1579 \left(\frac{\text{difficult
               words}}{\text{words}}
               \times100 \right)
               + 0.0496
               \left(\frac{\text{words}}{\text{sentences}}\right)$
               <br>
               <br>
               Where difficult words are words not present in a list
               of 763 words (updated to 3k words in 1995).
            </p>
          </section>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li>Definition: what is readability?</li>
              <li>Research statement</li>
              <li><strong class="text-primary">Related works:</strong>
                <ul>
                  <li>Readability formulas</li>
                  <li><strong class="text-primary">Machine learning</strong></li>
                  <li>Simple Wikipedia</li>
                </ul>
                <li>Idea</li>
                <li>Corpus creation</li>
                <li>Phrase lexicon learning</li>
                <li>Fine-grained readability measure</li>
                <li>Text rewriting</li>
                <li>Future works</li>
                <li>Conclusion</li>
            </ul>
          </section>
          <section>
            <h2>The tasks</h2>
            <style>
              .table th, .table td {
              text-align: center !important;
              }
            </style>
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>Classical task</th>
                  <th>Machine learning task</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Score a text</td>
                  <td>Regression</td>
                </tr>
                <tr>
                  <td>Sort texts on readability</td>
                  <td>Regression, classification on pairs of
                    documents</td>
                </tr>
                <tr>
                  <td>Assign a required grade to a text</td>
                  <td>Classification with grades as labels</td>
                </tr>
                <tr>
                  <td>Regroup texts of similar readability</td>
                  <td>Clustering</td>
                </tr>
              </tbody>
            </table>
          </section>
          <section>
            <h2>Language model approach</h2>
            <p>Schwarm and Ostendorf, 2005:</p>
            <br>
            <ul>
              <li>bigrams and trigrams LM alone</li>
              <li>combination in a SVM of:
                <ul>
                  <li>LM perplexities</li>
                  <li>readability formulas</li>
                  <li>syntactic features</li>
                </ul>
              </li>
            </ul>
          </section>
          <section>
            <h2>Complex features</h2>
            <p>
              Pitler and Nenkova, 2008:
            </p>
            <br>
            <ul>
              <li>unigram model</li>
              <li>lexical cohesion (cosine similiarity averaged over
                all sentences)</li>
              <li>syntactic features (as Schwarm and Ostendorf)</li>
              <li>entity coherence (analyse the subjects / objects of
                consecutive sentences)</li>
              <li>language model over discourse relations</li>
            </ul>
            <br>
            <br>
            <p>→ proves the superiority of discourse relations over
            average lengths of sentences and
            words. <strong>But</strong> discourse relations are not
            yet easily computable.</p>
          </section>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li>Definition: what is readability?</li>
              <li>Research statement</li>
              <li><strong class="text-primary">Related works:</strong>
                <ul>
                  <li>Readability formulas</li>
                  <li>Machine learning</li>
                  <li><strong class="text-primary">Simple Wikipedia</strong></li>
                </ul>
                <li>Idea</li>
                <li>Corpus creation</li>
                <li>Phrase lexicon learning</li>
                <li>Fine-grained readability measure</li>
                <li>Text rewriting</li>
                <li>Future works</li>
                <li>Conclusion</li>
            </ul>
          </section>
          <section>
            <h2>Simple English Wikipedia (SEW)</h2>
            <ul>
              <li>translation of the English Wikipedia in simple English</li>
              <li>goal is:
                <ul>
                  <li>to use only the 1000 most common words in English</li>
                  <li>to keep sentences short</li>
                </ul>
              </li>
              <li>100 000 articles</li>
            </ul>
            <br>
            <br>
            <strong>→ Amazing readability resource</strong>
          </section>
          <section>
            <h2>Comparable corpus
              <br><strong>EW</strong> → <strong>SEW</strong></h2>
            <p>(Zhu et al., 2010)</p>
            <ul>
              <li>align EW and SEW versions of a same article</li>
              <li>gather the differences to compute a non-readable → readable corpus</li>
              <li>100 000 pairs of sentences</li>
            </ul>
          </section>
          <section>
            <h2>Parallel corpus
              <br><strong>EW → EW & SEW</strong> → <strong>SEW</strong></h2>
            <p>(Yatskar et al., 2010)</p>
            <ul>
              <li>use EW and SEW revision histories</li>
              <li>gather the differences between two versions modified for readability</li>
            </ul>
          </section>
        </section>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li>Definition: what is readability?</li>
              <li>Research statement</li>
              <li>Related works:
                <ul>
                  <li>Readability formulas</li>
                  <li>Machine learning</li>
                  <li>Simple Wikipedia</li>
                </ul>
                <li><strong class="text-success">Idea</strong></li>
                <li>Corpus creation</li>
                <li>Phrase lexicon learning</li>
                <li>Fine-grained readability measure</li>
                <li>Text rewriting</li>
                <li>Future works</li>
                <li>Conclusion</li>
            </ul>
          </section>
          <section>
            <h2>Machine translation
              <br>to the rescue</h2>
            <ul>
              <li>use phrase based machine translation to model
                lexical improvements</li>
              <li>use translation scores as indicators of readability</li>
            </ul>
            <br>
            <br>
            <p>→ achieve fine-grained readability</p>
          </section>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li>Definition: what is readability?</li>
              <li>Research statement</li>
              <li>Related works:
                <ul>
                  <li>Readability formulas</li>
                  <li>Machine learning</li>
                  <li>Simple Wikipedia</li>
                </ul>
                <li>Idea</li>
                <li><strong class="text-primary">Corpus creation</strong></li>
                <li>Phrase lexicon learning</li>
                <li>Fine-grained readability measure</li>
                <li>Text rewriting</li>
                <li>Future works</li>
                <li>Conclusion</li>
            </ul>
          </section>
          <section>
            <h2>Corpus creation</h2>
            <h3>a subtask</h3>
            <p>Free readability corpora are based on comparable
              corpora. We propose a free corpus based on revision
              history.</p>
            <br>
            <p>Reasons:</p>
            <ul>
              <li>general process</li>
              <li>easily extensible outside of wikipedia (copy-editing)</li>
              <li>allows us to also make tooling available</li>
            </ul>
          </section>
          <section>
            <h2>Corpus creation</h2>
            <h3>Methodology</h3>
            <p>We use previously known methods:</p>
            <ul>
              <li>SEW edit history</li>
              <li>align the sentences with a `diff` program</li>
              <li>release it to the public</li>
            </ul>
          </section>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li>Definition: what is readability?</li>
              <li>Research statement</li>
              <li>Related works:
                <ul>
                  <li>Readability formulas</li>
                  <li>Machine learning</li>
                  <li>Simple Wikipedia</li>
                </ul>
                <li>Idea</li>
                <li>Corpus creation</li>
                <li><strong class="text-danger">Phrase lexicon learning</strong></li>
                <li>Fine-grained readability measure</li>
                <li>Text rewriting</li>
                <li>Future works</li>
                <li>Conclusion</li>
            </ul>
          </section>
          <section>
            <h2>Lexicon creation</h2>
            <p>The objective is to score the tranlations in the corpus
              we created: go from a corpus $\mathcal{C} \in
              \mathcal{P} \times \mathcal{P}$ to a lexicon
              $\mathcal{L} \in \mathcal{P} \times \mathcal{P} \times
              \mathbb{R}$ ($\mathcal{P}$ is the set of English phrases)</p>
            <br>
            <p>→ need to define score functions $\mathcal{P} \times \mathcal{P} \rightarrow \mathbb{R}$</p>
          </section>
          <section>
            <h2>Properties of good scores</h2>
            <p>Since we are looking for good lexical simplifications:</p>
            <ul>
              <li>the less common $x$ is, the better $\mathcal{S}(x, y)$ should be</li>
              <li>the more common $y$ is, the better $\mathcal{S}(x, y)$ should be</li>
              <li>the more common $(x, y)$ is in $\mathcal{C}$, the
              better $\mathcal{S}(x, y)$ should be</li>
            </ul>
          </section>
          <section>
            <h2>How rare is $x$ ?</h2>
            <p>Direct language model score is not enough ($P_{LM}\big(x\big)$):</p>
            <ul>
              <li>“exhausted” would likely have a higher LM score than “I am”</li>
              <li>we want to rewrite “exhausted”, not “I am”</li>
            </ul>
            <p>→ average by $x$ length: $\sqrt[\lvert x \rvert]{P_{LM}\big(x\big)}$</p>
          </section>
          <section>
            <h2>How common is $y$ ?</h2>
            <p>Two worthy definitions:</p>
            <ul>
              <li>$P_{LM}\big(y\big)$ to allow only for short simplifications</li>
              <li>$\sqrt[\lvert y \rvert]{P_{LM}\big(y\big)}$ to allow for paraphrases</li>
            </ul>
          </section>
          <section>
            <h2>How common is
              <br>$(x, y)$ in $\mathcal{C}$ ?</h2>
            <p>We can answer with two probabilities:</p>
            <ul>
              <li>$P_\mathcal{C}\big((x, y)\big) =
                \frac{\lvert\left\{(x, y) \,\middle|\, (x, y) \in
                \mathcal{C}\right\}\rvert}{\lvert\mathcal{C}\rvert}$</li>
              <li>$P_\mathcal{C}\big((x, y)\big|x\big) =
                \frac{\lvert\left\{(x, y) \,\middle|\, (x, y) \in
                \mathcal{C}\right\}\rvert}{\lvert\left\{(x, y')
                \,\middle|\, y' \in \mathcal{P} \land (x, y') \in
                \mathcal{C}\right\}\rvert}$</li>
            </ul>
          </section>
          <section>
            <h2>Combinations</h2>
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>score</th>
                  <th>x is conditional?</th>
                  <th>uses LM of y?</th>
                  <th>normalizes LM of y?</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>$\mathcal{S}$</td>
                  <td>✗</td>
                  <td>✗</td>
                  <td>✗</td>
                </tr>
                <tr>
                  <td>$\mathcal{S}_c$</td>
                  <td>✓</td>
                  <td>✗</td>
                  <td>✗</td>
                </tr>
                <tr>
                  <td>$\mathcal{S}_d$</td>
                  <td>✗</td>
                  <td>✓</td>
                  <td>✗</td>
                </tr>
                <tr>
                  <td>$\mathcal{S}_{dc}$</td>
                  <td>✓</td>
                  <td>✓</td>
                  <td>✗</td>
                </tr>
                <tr>
                  <td>$\mathcal{S}_{wd}$</td>
                  <td>✗</td>
                  <td>✓</td>
                  <td>✓</td>
                </tr>
                <tr>
                  <td>$\mathcal{S}_{wdc}$</td>
                  <td>✓</td>
                  <td>✓</td>
                  <td>✓</td>
                </tr>
              </tbody>
            </table>
          </section>
          <section>
            <h2>Score $\mathcal{S}$</h2>
            <p>$\begin{align}
            \mathcal{S}(x, y) & = \log \frac{P_\mathcal{C}\big((x, y)\big)^{\lambda_1}}
            {\sqrt[\lvert x \rvert]{P_{LM}\big(x\big)}^{\lambda_2}} \\
            & = \lambda_1 \log P_\mathcal{C}\big((x, y)\big) - \lambda_2 \log
              \sqrt[\lvert x \rvert]{P_{LM}\big(x\big)} \end{align}$</p>
            <br>
            <ul>
              <li class="text-success"><strong>discovered → found</strong> has a good score</li>
              <li class="text-danger"><strong>and → .</strong> has a good score</li>
            </ul>
          </section>
          <section>
            <h2>Score $\mathcal{S}_c$</h2>
            <h3>$c$ for <strong>c</strong>onditional</h3>
            <p>$\mathcal{S}(x, y) = \log \frac{P_\mathcal{C}\big((x, y) \,\big|\, x\big)^{\lambda_1}}
              {\sqrt[\lvert x \rvert]{P_{LM}\big(x\big)}^{\lambda_2}}$</p>
            <br>
            <ul>
              <li class="text-success"><strong>and → .</strong> has a bad score</li>
              <li class="text-danger"><strong>targeted → wanted to vote off</strong> has a good score</li>
            </ul>
          </section>
          <section>
            <h2>Score $\mathcal{S}_d$</h2>
            <h3>$d$ for <strong>d</strong>ouble language model</h3>
            <p>$\mathcal{S}(x, y) = \log \frac{P_\mathcal{C}\big((x, y)\big)^{\lambda_1}
            P_{LM}\big(y\big)^{\lambda_3}}
              {\sqrt[\lvert x \rvert]{P_{LM}\big(x\big)}^{\lambda_2}}$</p>
            <p class="text-danger">non-content rewritings have high scores. Might be fixable with $\lambda$ tweaking.</p>
          </section>
          <section>
            <h2>Score $\mathcal{S}_{dc}$</h2>
            <p>$\mathcal{S}(x, y) = \log \frac{P_\mathcal{C}\big((x, y) \,\big|\, x\big)^{\lambda_1}
            P_{LM}\big(y\big)^{\lambda_3}}
              {\sqrt[\lvert x \rvert]{P_{LM}\big(x\big)}^{\lambda_2}}$</p>
          </section>
          <section>
            <h2>Score $\mathcal{S}_{wd}$</h2>
            <h3>$w$ for <strong>w</strong>eighted replacement</h3>
            <p>$\mathcal{S}(x, y) = \log \frac{P_\mathcal{C}\big((x, y)\big)^{\lambda_1}
            \sqrt[\lvert y \rvert]{P_{LM}\big(y\big)}^{\lambda_3}}
              {\sqrt[\lvert x \rvert]{P_{LM}\big(x\big)}^{\lambda_2}}$</p>
            <ul>
              <li class="text-success"><strong>good score: </strong>considerable → a lot of</li>
              <li class="text-danger"><strong>bad score: </strong>discovered → found</li>
            </ul>
          </section>
          <section>
            <h2>Score $\mathcal{S}_{wdc}$</h2>
            <p>$\mathcal{S}(x, y) = \log \frac{P_\mathcal{C}\big((x, y) \,\big|\, x\big)^{\lambda_1}
            \sqrt[\lvert y \rvert]{P_{LM}\big(y\big)}^{\lambda_3}}
              {\sqrt[\lvert x \rvert]{P_{LM}\big(x\big)}^{\lambda_2}}$</p>
          </section>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li>Definition: what is readability?</li>
              <li>Research statement</li>
              <li>Related works:
                <ul>
                  <li>Readability formulas</li>
                  <li>Machine learning</li>
                  <li>Simple Wikipedia</li>
                </ul>
                <li>Idea</li>
                <li>Corpus creation</li>
                <li>Phrase lexicon learning</li>
                <li><strong style="color: black;">Fine-grained readability measure</strong></li>
                <li>Text rewriting</li>
                <li>Future works</li>
                <li>Conclusion</li>
            </ul>
          </section>
          <section>
            <h2>A general framework</h2>
            Recursive combination of readability scores with 3 functions:
            <ul>
              <li>$\pi$ to handle the different scores of a sentence <strong>p</strong>art</li>
              <li>$\sigma$ to handle the parts of a <strong>s</strong>entence</li>
              <li>$\theta$ to handle the sentences of a <strong>t</strong>ext</li>
            </ul>
            $\begin{align*}
            f(t) = \theta\Bigg(& \Bigg\{ \sigma \left(\left\{ \pi(s)| x \subset sent \land (x, y, s) \in \mathcal{L}\right\}\right)
             \\
            & \Bigg|\, \text{$sent$ is a sentence in $t$} \Bigg\}\Bigg)
            \end{align*}$
          </section>
          <section>
            <h2>An application</h2>
            <p>We construct $f_{max}$ with:</p>
            <ul>
              <li>$\pi = \max$</li>
              <li>$\sigma = \max$</li>
              <li>$\theta = \mathrm{average}$</li>
            </ul>
            <br>
            <br>
            <p>in English: $f_{max}$ averages the maximum of the
              sentences lexical improvement scores to assess the
              readability of a text.</p>
          </section>
          <section>
            <h2>Example</h2>
            <p>Given a text with the best transformation computed for
              each part of each sentence:</p>
            <img src="img/rrf-1.png" class="stretch">
          </section>
          <section>
            <h2>Example</h2>
            <p>Compute the sentence scores with $\max$:</p>
            <img src="img/rrf-2.png" class="stretch">
          </section>
          <section>
            <h2>Example</h2>
            <p>Compute the text score with $\mathrm{average}$:</p>
            <img src="img/rrf-3.png" class="stretch">
          </section>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li>Definition: what is readability?</li>
              <li>Research statement</li>
              <li>Related works:
                <ul>
                  <li>Readability formulas</li>
                  <li>Machine learning</li>
                  <li>Simple Wikipedia</li>
                </ul>
                <li>Idea</li>
                <li>Corpus creation</li>
                <li>Phrase lexicon learning</li>
                <li>Fine-grained readability measure</li>
                <li><strong class="text-success">Text rewriting</strong></li>
                <li>Future works</li>
                <li>Conclusion</li>
            </ul>
          </section>
          <section>
            <h2>Text rewriting</h2>
            We can use our lexicon to rewrite text: with <strong>weighted transducers</strong>.
          </section>
          <section>
            <h2>Weighted Transducers</h2>
            <h3>definition</h3>
            <ul>
              <li>$\Sigma$ input alphabet</li>
              <li>$\Delta$ output alphabet</li>
              <li>$Q$ set of states</li>
              <li>$I \subseteq Q$ set of initial states</li>
              <li>$F \subseteq Q$ set of final states</li>
              <li>$E \subseteq Q \times (\Sigma \cup \{\varepsilon\})
                \times (\Delta \cup \{\varepsilon\}) \times \mathbb{K}
                \times Q$ set of transitions</li>
              <li>$\lambda : I \rightarrow \mathbb{K}$ the initial weight function</li>
              <li>$\rho : F \rightarrow \mathbb{K}$ the final weight function mapping
                $F$ to $\mathbb{K}$</li>
            </ul>
          </section>
          <section>
            <h2>Weighted Transducers</h2>
            <h3>construction</h3>
            <ul>
              <li>$\Sigma = \mathcal{T}$</li>
              <li>$\Delta = \mathcal{T}$</li>
              <li>$Q = \left\{ q_i | 0 \leq i \leq n \right\}$</li>
              <li>$I = \{ q_0 \}$</li>
              <li>$F = \{ q_n \}$</li>
              <li>$\displaystyle
                \begin{aligned}[t]
                E & = \left\{ (q_{i - 1}, x, y, s, q_i) | (x, y, s) \in \mathcal{L}
                \land 1 \leq i \leq n \right\} \\
                & \quad \, \cup \left\{ (q_{i - 1}, t_i, t_i, \mathbb{1}, q_i)
                | 1 \leq i \leq n \right\}
                \end{aligned}$</li>
              <li>$\lambda : x \mapsto \mathbb{1}$</li>
              <li>$\rho : x \mapsto \mathbb{1}$</li>
            </ul>
          </section>
          <section>
            <h2>Weighted Transducers</h2>
            <h3>example</h3>
            <img src="img/monoids.svg" width="830">
          </section>
          <section>
            <h2>Weighted Transducers</h2>
            <h3>Combining weights</h3>
            <table class="table table-striped">
              <thead>
                <tr>
                  <th>score with +</th>
                  <th>score with min</th>
                  <th>output</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>0</td>
                  <td>0</td>
                  <td>I'm exhausted</td>
                </tr>
                <tr>
                  <td>-0.9</td>
                  <td>-0.9</td>
                  <td>I'm tired</td>
                </tr>
                <tr>
                  <td>-3.2</td>
                  <td>-3.2</td>
                  <td>I am exhausted</td>
                </tr>
                <tr>
                  <td>-4.1</td>
                  <td>-3.2</td>
                  <td>I am tired</td>
                </tr>
              </tbody>
            </table>
            <img src="img/monoids.svg">
          </section>
        </section>
        <section>
          <section>
            <h1>Overview</h1>
            <ul>
              <li>Definition: what is readability?</li>
              <li>Research statement</li>
              <li>Related works:
                <ul>
                  <li>Readability formulas</li>
                  <li>Machine learning</li>
                  <li>Simple Wikipedia</li>
                </ul>
                <li>Idea</li>
                <li>Corpus creation</li>
                <li>Phrase lexicon learning</li>
                <li>Fine-grained readability measure</li>
                <li>Text rewriting</li>
                <li><strong class="text-warning">Future works</strong></li>
                <li>Conclusion</li>
            </ul>
          </section>
          <section>
            <h2>Evaluation</h2>
            <ul>
              <li>manual evaluation, comparison between top rewritings
                of this approach and other lexical approaches</li>
              <li>automatic evaluation, with readability formulas or
                machine learning approaches</li>
            </ul>
          </section>
          <section>
            <h2>Syntactic rewriting</h2>
            <p>Go from string → string transducing to tree → tree
              transducing to handle syntactic rewritings.</p>
          </section>
        <section>
          <h1>Conclusion</h1>
          What we have talked about today:
          <ul>
            <li>a new readability corpus and related tools</li>
            <li>an adaptation of machine translation techniques to
              readability (phrase translation scoring)</li>
            <li>a way to derive a fine-grained readability measure
              from it</li>
            <li>how to also apply it to text rewriting</li>
            <li>a tool to conquer them all, Readability Lab</li>
          </ul>
        </section>
        <section>
          <h2>Slightly off-topic ♥</h2>
          <img src="img/fuji.jpg">
          
        </section>
        <section>
          <h1><span style="color: white;">Thank</span>
            <span style="color: black;">you</span>
            <span style="color: blue;">very</span>
            <span style="color: red;">much</span>
            <span style="color: yellow;">for</span>
            <span style="color: purple;">your</span>
            <span style="color: pink;">attention</span>
            <br><span class="text-primary">☕</span></h1>
        </section>
        <section>
          <section>
            <h1><span style="color: black;">Do you have any
              question?</span></h1>
          </section>
        </section>
      </div>
    </div>
    
    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>
    
    <script>
      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
       controls: false,
       progress: true,
       history: true,
       center: true,
       slideNumber: true,
       
       theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
       transition: 'none', // default/cube/page/concave/zoom/linear/fade/none
       backgroundTransition: 'none', // default/cube/page/concave/zoom/linear/fade/none
       math: {
         mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
         config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
       },
       // Parallax scrolling
       // parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
       // parallaxBackgroundSize: '2100px 900px',
       
       // Optional libraries used to extend on reveal.js
       dependencies: [
         { src: 'plugin/math/math.js', async: true },
         { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
         { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
         { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
         { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
         { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
         { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
       ]
     });
     
    </script>
    <script src="bootstrap/js/bootstrap.min.js"></script>
    
  </body>
</html>
