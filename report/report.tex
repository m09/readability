\documentclass[a4paper, 11pt, onepage]{scrreprt}

\usepackage{fontspec}
\setmainfont{Linux Libertine O}
\usepackage{polyglossia}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{mathtools}
\usepackage{bbold}
\usepackage[hyperref]{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{numprint}
\usepackage{hyperref}
\usepackage{siunitx}

\newcommand\wiki{\textsc{Wikipedia}}
\newcommand\ew{\textsc{English Wikipedia}}
\newcommand\sew{\textsc{Simple English Wikipedia}}
\newcommand\tableref[1]{\hyperref[#1]{Table \ref*{#1}}}
\newcommand\figureref[1]{\hyperref[#1]{Figure \ref*{#1}}}
\newcommand\sectionref[1]{\hyperref[#1]{Section \ref*{#1}}}
\newcommand\chapterref[1]{\hyperref[#1]{Chapter \ref*{#1}}}
\newcommand\equaref[1]{\hyperref[#1]{Equation \ref*{#1}}}
\newcommand\maps[1]{\xrightarrow{\mathcal{#1}}}
\newcommand\card[1]{\lvert #1 \rvert}
\newcommand\suchthat{\, \middle| \,}
\newcommand\given{\, \middle| \,}
\newcommand\proba[2][]{P_{#1} \left( #2 \right)}

\DeclareMathOperator*{\argmin}{\arg\,\min}
\DeclareMathOperator*{\argmax}{\arg\,\max}

\renewcommand\chaptername{Section}
\renewcommand\thechapter{\Roman{chapter}}

\DeclareGraphicsExtensions{.png, .jpeg, .jpg, .svg, .eps, .pdf}
\graphicspath{{./img/}}

\hypersetup{
  colorlinks = true,
  linkcolor = violet,
  urlcolor = teal,
  citecolor = gray
}

\begin{document}
\begin{titlepage}
  \begin{center}
    \noindent\rule{\textwidth}{0.4pt}
    {\huge\bfseries Improving text readability using\\
      \sew\\}
    \noindent\rule{\textwidth}{0.4pt}\\
    % ----------------------------------------------------------------
    \vspace{1.5cm}
    {\large
    \begin{tabularx}{\textwidth}{lXll}
      \textbf{Author:} & \textsc{Hugo Mougard} &
      \textbf{Director:} & \textsc{Akiko Aizawa} \\
      \textbf{Tutor:} & \textsc{Colin de la Higuera} &
      \textbf{Advisor:} & \textsc{Pascual Martínez-Gomez} \\
    \end{tabularx}}\\[5pt]
    % ----------------------------------------------------------------
    \vfill
    \parbox{0pt}{\large \begin{tabbing}
        \textbf{Sending institution:} \= National Institute of Informatics \kill
        \textbf{Sending institution:} \> \textsc{Université de Nantes, France} \\
        \textbf{Hosting institution:} \> \textsc{National Institute of
        Informatics, Japan} \\
      \end{tabbing}}
    \vfill
    {\LARGE ATAL Master 2 internship report}
    \vfill
    % ----------------------------------------------------------------
    \includegraphics[width=0.16\textwidth]{img/nii.png}
    \hspace{0.03\textwidth}\hfill
    \includegraphics[width=0.19\textwidth]{img/lina.png}
    \hfill
    \includegraphics[width=0.19\textwidth]{img/univ.eps}
    \vfill
    {July 2014}
  \end{center}
\end{titlepage}

\tableofcontents

\chapter{Acknowledgments}

I would like to start those acknowledgments by thanking Aizawa-sensei
and Katsu-san for their precious help before our arrival to Japan, and
their tireless support during our stay, this was greatly appreciated.

Meeting a solid number of great people has been a pleasure during this
internship. I thank especially Fujinuma-san and Adelin for their
support (and corrections to this report for the latter).

I also wouldn't have been able to conduct this internship without the
scientific advising of Aizawa-sensei and Pascual-san, I thank them for
all their good insight, even though I am convinced I didn't make the
best of it.

And it was very nice to have someone exterior to discuss ideas with
when I wanted to, I thank Colin de la Higuera for that.

Because this internship is also the end of my Master's program, I
would like to acknowledge the awesomeness of the ATAL team (students
and teachers alike!). Working with passionate people in this context
has been a pleasure for now close to two years.

Lastly but not leastly—please forgive my English, I would like to
thank Grégoire for about everything during those five months of
internship, it was great to share this Japanese experience with a
friend (and still will be for about three weeks at the
time of this writing).

\chapter{Laboratory presentation}

\section{\textsc{National Institute of Informatics}}
\label{sec:national-institute-of-informatics}

The \textsc{National Institute of Informatics} is a Japanese research
institute located in Chiyoda-ku, Tokyo. It has strong connections with
the \textsc{University of Tokyo} (\textsc{Todai}) and welcomes
computer scientists of numerous domains to form a strong innovative
pole in the Japanese computer science research landscape.

Its International Exchange Agreements program allowed myself and
another student of the first promotion of the ATAL master to join for
a five month internship.

\section{\textsc{Aizawa Laboratory}}
\label{sec:aizawa-laboratory}

The team that welcomed me for this work is \textsc{Aizawa
  Laboratory}\footnote{\url{http://www-al.nii.ac.jp/en/}}. It is led
by Professor \textsc{Akiko
  Aizawa}\footnote{\url{http://research.nii.ac.jp/~akiko/index_e.html}},
the director of this internship. At the time of this writing, the
laboratory has 18 members and strong partnerships with past members of
the laboratory.

Its expertise lies in several sub-domains of Natural Language
Processing (NLP):
\begin{itemize}
\item NLP using gaze information;
\item analysis and mining of scientific papers;
\item mathematical information retrieval;
\item syntactic and semantic structure analysis of natural language
  text;
\item extraction and classification of technical terms.
\end{itemize}

The work presented in this report has been done in the group
interested in NLP using gaze information, mostly for historical
reasons: the work proposed during this internship doesn't use gaze
information. Still, readability — the topic of this internship — has
strong connections to gaze NLP and other members of the group were
very knowledgeable in this domain. This was a great opportunity for
the development of this internship.

\chapter{Introduction}

Whether it is to teach children how to read, to assess the
comprehensiveness of technical manuals or to better grasp how we
understand things, readability has been studied for about two
centuries for its impact in both engineering and scientific endeavors.

Despite the interest of the scientific community for many aspects of
readability—including cognitive and social ones, early research was
mostly focused on finding basic metrics to measure how understandable
a text is. It is only in recent years that scientists have been able
to come up with more interesting methods, thanks in particular to the
advances in machine learning, gaze processing, the new speed of modern
computers and the unparalleled amount of data available on the
internet.

The high-level goal of this internship is to investigate these methods
and improve on them. To be more specific, most methods consider
documents as a whole when analyzing their readability, which makes it
hard for users to understand which parts are the most important to
rework. We aim at providing a fine-grained analysis of readability in
this work. This shift has been observed in recent years in other
natural language processing domains, such as sentiment analysis with
the recursive approach proposed by \cite{socher2013recursive} that
allows users to detect which part of a sentence conveys which
sentiment. In the same way, we want to detect which part of a sentence
is readable or non readable, and for which reasons.

\chapter{Related work}
\label{cha:sota}

This section details the history and state of the art of computational
readability study that served as a basis for this work.

\section{Early works and readability formulas}
\label{sec:early-works-and-formulas}
Readability has been studied extensively. The first works in this area
date back to more than a century ago: in 1893, Sherman published a
book \cite{sherman1893analytics} where he compared modern English and
English spoken four centuries before with considerations that are very
alike the ones that current readability studies outline. Then, in
1921, Thorndike computed a list of ten thousand easy-to-read words
\cite{thorndike1921teacher}. This list got used shortly thereafter by
teachers trying to select good books for children learning to read
\cite{lively1923method}: they went through the process of estimating
the readability of many books thanks to a basic formula and
Thorndike's list.

This was the beginning of an important branch of research focusing on
how to best estimate the readability of a text thanks to simple
formulas. Among the most famous methods, it is interesting to mention
the Flesch Reading Ease \cite{flesch1948new} which introduced the use
of a combination of the average number of words per sentence and the
average number of syllables per word to estimate the readability of a
text. Most of the subsequent approaches also use those metrics. This
method yields a 0.91 correlation with text understanding and has been
used outside of research to improve the readability of a vast amount
of publications, yielding excellent results in readership increase.

Roughly at the same time, Dale and Chall introduced a readability
formula that makes use of a list a words instead of the average number
of syllables per word to estimate readability
\cite{dale1948formula}. This more precise definition of world
difficulty allows this metric to reach a 0.93 correlation with text
understanding and because of that has been one of the most used
formulas in research.

Later works brought formulas that are easier to compute or give
slightly better results. Some also give their result as the grade that
would be required to be able to read the input text. Overall are all
using the same variables as either Flesch or Dale and Chall
\cite{mclaughlin1969smog, kincaid1975derivation,
  chall1995readability}.

Despite their useful applications, readability formulas are not
perfect. Some works have detailed their problems convincingly
\cite{duffy1985readability, schriver2000readability}. Among the most
important ones is the imprecision of the features used: of the
following sentences, (1.)  is often considered easier to understand
than (2.), despite being lengthier:
\begin{enumerate}
\item The mouse ate the cheese, and then the rat ate the mouse, and
  after that, the cat ate the rat and died.
\item The cat that ate the rat that ate the mouse that ate the cheese
  died.
\end{enumerate}
That showcases that sentence length should be considered with care
when trying to improve on existing readability measures. The average
word length—one of the other dominant features—has similar
problems. The reasons for that is two-fold:
\begin{itemize}
\item affixes are known even to early readers;
\item some very short words are difficult intrinsically.
\end{itemize}
To illustrate that we can compare “curr” (to make a murmuring sound)
and “reinventing”. Due to the rarity of “curr”, most early readers
will have troubles with it while “reinventing” will often not be
problem because of the known meanings of the “re-” and “-ing” affixes.

Finally, the fact that many formulas only consider short windows of a
text to estimate its readability make them not reliable without
repetition of the measure at several points in a text and an averaging
(which is what Dale \& Chall does by default).

To address those shortcomings, scientists have recently proposed
machine learning based approaches. The next section details those
works.

\section{Machine Learning approaches}
\label{sec:ml-approaches}

Readability assessment can be seen as a supervised classification
task. To predict the required grade to understand a given input text,
\cite{collins2004language} use a unigram language model of each target
grade in order to see which one is the most likely to generate the
input.

\cite{schwarm2005reading} improve on this approach by using a trigram
model instead of a unigram model. They also incorporate classical
readability features by using a SVM with the perplexity scores of the
language models as features along with readability formula scores and
various syntactic features.

In 2008, Pitler and Nenkova conduct a study where they further
investigate the use of complex features to feed machine learning
readability approaches \cite{pitler2008revisiting}. They study
features such as discourse relations and entity coherence together
with advanced syntactic and lexical features. They notice possible
improvements but conclude that until robust automatic method to obtain
the more advanced features exist, it is unlikely that people will
succeed in using them in a large scale, automated system.

With machine learning approaches becoming successful, gathering data
became a crucial point of readability research. In the next section we
see how researchers have used \wiki{} to conduct their machine
learning research in readability.

\section{Readability research using \wiki}
\label{sec:wiki-approaches}

From 2006 onwards, \sew has been rightly seen as an important resource
for readability studies. Scientists have used it to compute parallel
corpora of readable sentences and their hard-to-read counterparts.

\chapter{Work}
\label{cha:work}

To achieve fine-grained readability analysis, we propose an approach
relying on statistical phrase based translation
\cite{koehn2003statistical}. The underlying idea is to build a lexicon
of not-so-readable phrases and their readable counterparts enriched
with a score reflecting the quality of the translation to be able to
score areas of a text depending on the translations available for
them. \sew{} is used to provide the initial translations.

In the literature, most of the best models are based on a language
model of the target language. Our approach differs from this principle
mainly because \sew{} doesn't contain a sufficient number of examples
so that we can reliably build readable English and non-readable
English language models.

The work done during this internship is better presented as five
different complementary tasks.

\section{Automatic readability parallel corpus creation}
\label{sec:corpus}

Machine learning approaches are used more and more and in a wide range
of scientific domains for their effectiveness. Most of the time, the
more data you have, the best your results will be, up to an extent
that people described data efficiency as “unreasonable”
\cite{halevy2009unreasonable}. In this context, it seems that being
able to automatically gather data is of utmost importance. Moreover,
raw data is often not enough. Most techniques require a lengthy and
costly annotation process. For this reason, researchers in every
domain using machine learning invest time finding clever techniques to
automatically obtain annotated corpora.

In readability, such a clever technique is to leverage the power of
\wiki, and in particular \sew. To do so, mainly two techniques have
been used: the first is to align the articles present both in the
regular \ew{} and in \sew{} to build a parallel corpus. This approach
has been used with great success in \cite{zhu2010monolingual} and has
resulted in the
\textsc{PWKP-108016}\footnote{\url{https://www.ukp.tu-darmstadt.de/data/sentence-simplification/simple-complex-sentence-pairs/}}
dataset. The second is to use the revision history of \sew, as
showcased in \cite{yatskar2010sake}. In this work we use the second
approach.

The main rationale for not using the existing \textsc{PWKP-108016}
corpus and instead creating one of our own is that we believe the
technique used to automatically construct the corpus from a revision
history is a general process and can therefore be extended to many
data sources beyond \wiki. Hence the community would benefit from an
efficient tool to exploit such sources. Those possible extensions
include many of the processes of the professional publishing world and
especially copy editing. If proven efficient, big corpora could be
gathered from journalists, writers, scientists, etc. On the contrary,
it has been shown to be extremely hard to find good comparable corpora
outside of \wiki{} and some restricted institution publications such
as the European Parliament proceedings on which \textsc{Europarl} is
based \cite{koehn2005europarl}, which makes the first approach hard to
generalize. It is worth noting that it might be even harder to
construct such corpora for readability study than for machine
translation, due to the lesser number of publications requiring
different levels of readability than the number of publications that
need to be translated. Finally, the outputs made public by
\cite{yatskar2010sake} are of high quality but not intended to be used
as a corpus and rather as a supplementary material to study their
approach.

To build automatically our corpus, we leverage the fact that when
editing \sew, contributors have the possibility to write a commit
message, so that other people can quickly browse history and guess the
role of a particular commit. This allows us to automatically detect if
a particular commit was intended to fix a readability issue or was
serving another purpose (improving the content of the article,
cleaning the wiki, vandalizing, …). This usage of metadata is not
novel, it was used in the \textsc{Simpl} approach presented in
\cite{yatskar2010sake}. Though, we fine-tune the heuristic used to
address some problems encountered in this article and to harden the
process. An example of readability edit can be seen in
\figureref{fig:dan-kelly}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{dan-kelly}
  \caption{An edit of the Dan Kelly article with the commit message
    “use simple english here”}
  \label{fig:dan-kelly}
\end{figure}

The heuristic we use to gather edits is extremely simple: once
stripped of the category information, we only consider edits with
commit messages $m$ so that:

\[
m \in \left\{\text{“simplify”}, \text{“simplifying”},
  \text{“simplified”}, \text{“simplification”},
  \text{“simpler”}\right\}
\]

This may seem drastic but we believe it is necessary due to the many
noisy commits introduced with broader rules. To illustrate this point,
we can consider the commit messages of
\tableref{tab:problematic-commits}. We can see that commit \#1 has two
purposes, one of which will bring noise (the wikification). Commit \#2
contains one of the matching terms, but we should not consider
it. It's not clear if commit \#3 tries to address readability or not.

\begin{table}[H]
  \centering
  \caption{Problematic commit messages}
  \begin{tabular}{rp{12cm}}
    \toprule
    \# & Commit message \\
    \midrule
    1 & “simplified, and wikified” \\
    \addlinespace
    2 & “wikify. needs simplifying” \\
    \addlinespace
    3 & “exhaustive, to help us work out titles of articles - lots of
    discussion here, and no, this is not simple engouh yet” \\
  \end{tabular}
  \label{tab:problematic-commits}
\end{table}

Furthermore, we determined by an empirical study that those commits
represented an important part of the commits we might have retrieved
with a broader rule, so we simply did not consider them.

The next step for the creation of the corpus is to compute the precise
edit that the contributor made. To do so, tokenization and sentence
splitting both were performed on the original version of the article
and its revision using
\textsc{OpenNLP}\footnote{\url{https://opennlp.apache.org/}}. Then the
two texts were aligned at the world and sentence levels with Myers'
alignment algorithm—the algorithm used in git to compute diffs
\cite{myers1988optimal}. We rely on the implementation proposed by
\textsc{jgit}\footnote{\url{http://www.eclipse.org/jgit/}}.

Keeping re-usability in mind, the implementation uses
\textsc{uimaFIT}, a standard NLP framework
\cite{ogren-bethard:2009:SETQA-NLP}. The complete pipeline is
available on
\textsc{Github}\footnote{\url{https://github.com/m09/readability}}. It
is freely re-usable thanks to its permissive license.

The obtained corpus has important noise problems. A quick glance at
\tableref{tab:problematic-edits} is sufficient to understand that some
edits are not readability edits, but either alignment deficiencies or
edits serving complementary roles. When filtered to only consider
revisions of 5 words at most, to match the policy used in
\cite{yatskar2010sake}, the number of entries is reduced to
\numprint{23507}.

\begin{table}[H]
  \centering
  \caption{Problematic edits}
  \begin{tabular}{rp{6cm}p{6cm}}
    \toprule
    \# & Original version & Revised version \\
    \midrule
    1 & , and & . she also used them in her arguments about \\
    \addlinespace
    2 & 2010 federal election, is russell matheson, a member & size\\
  \end{tabular}
  \label{tab:problematic-edits}
\end{table}

The unfiltered corpus has \numprint{43753} entries. However, it has
important noise problems. A quick glance at
\tableref{tab:problematic-edits} is sufficient to understand that some
edits are not readability edits, but either alignment deficiencies or
edits serving complementary roles. When filtered to only consider
revisions of 5 words at most, to match the policy used in
\cite{yatskar2010sake}, the number of entries is reduced to
\numprint{35927}. Of those \numprint{35927} couples $(\mathtt{original},
\mathtt{revision})$, \numprint{25675} occur only once and
\numprint{18330} $\mathtt{original}$s have only one readable
equivalent.

The resulting resource is enriched with part-of-speech, and syntactic
parsing annotations and is proposed in an XML format.

\section{Phrasal lexicon creation}
\label{sec:lexical-enhancements}

With this corpus at our disposal, it is we next define a way to use it as a
lexical dictionary to simplify vocabulary in a text. The goal is to
allow authors to pick simpler words than the ones they used, in an
efficient manner. In what follows, we will denote the multiset of all
rewritings of $x$ into $y$ in the dictionary $\mathcal{D}$ by $x
\maps{D} y$ and we will use $\_$ as a wildcard to denote any sequence
of words.

The first approach and the most basic one, given a dictionary
$\mathcal{D}$ and for a word $w$, is to propose as lexical rewriting
candidates the set $R_w$ such that:

\[
R_w = \left\{r \suchthat w \maps{D} r\right\}
\]

But obviously, while this approach will have a important recall, it
will be extremely noisy and will not be of any real help to an author,
the time lost cruising through the noise outweighing the benefits of
the correct lexical enhancements.

A first improvement is to use the part of speeches available in the
corpus to make sure they match. That will already somewhat reduce
lexical ambiguity. That allows us to refine the definition of $R_w$ as
follows, $p_w$ being the part of speech of $w$:

\begin{equation*}
  R_w = \left\{
    r \suchthat u \maps{D} r \land u = w \land p_u = p_w
  \right\}
\end{equation*}

This is not enough. It would be practical to be able to compare
rewritings and rank them from the worst to the best. Rewriting scoring
functions are introduced to achieve that.

Maybe the most obvious scoring function is one that gives a score to a
rewriting that reflects how often it occurs in the corpus
$\mathcal{D}$. The probability $\proba{w \maps{D} r}$ of a rewriting
from $w$ to $r$ occurring in $\mathcal{D}$ is computed as follows:

\begin{equation*}
  \proba{w \maps{D} r} = \frac{\card{w \maps{D} r}}{\card{\_ \maps{D} \_}}
\end{equation*}

\equaref{eq:socc} expresses the score function $\mathcal{S}_{OCC}$
that uses this probability and measures how often a rewriting occurs.

\begin{equation}
  \label{eq:socc}
  \mathcal{S}_{OCC}(w \maps{D} r) = \log \proba{w \maps{D} r}
\end{equation}

We take the log to ensure the numerical precision of the computations
despite the imprecision of floating point arithmetic for small values.

Even though this score is a great starting point, it is not a good
score to evaluate lexical rewritings. Its main shortcoming is that it
outlines mainly punctuation and non-content words rewritings. Those
are mainly linked to syntax and cannot be handled by a purely lexical
rewriting system at any acceptable level of performance. The following
scores try to fix this punctuation and non-content words focus to turn
it into a lexical focus.

To accomplish this shift, we define the quality of a rewriting of $w$
into $r$ as the probability of a human rewriting $w$ into $r$. By
combining a language model to the information present in the corpus
$\mathcal{D}$, it is possible to derive a measure close to this
probability.

The score $\mathcal{S}_{LM}$ is one way to capture the above
definition, with $\proba[LM]{w}$ the language model probability of
$w$:

\begin{align}
  \label{eq:slm1}
  \mathcal{S}_{LM}(w \maps{D} r) & = \log \frac{\proba{w \maps{D} r}^\lambda}%
  {\proba[LM]{w}^{(1 - \lambda)}} \\
  \label{eq:slm2}
  & = \lambda \mathcal{S}_{OCC} - (1 - \lambda) \log \proba[LM]{w}
\end{align}

The two equations above explain $\mathcal{S}_{LM}$ in two different
ways. \equaref{eq:slm1} best shows that $\mathcal{S}_{LM}$ is an
approximation of the ratio of the number of times a word is rewritten
on the number of times it appears. \equaref{eq:slm2} best shows that
$\mathcal{S}_{LM}$ is a balancing act: both $\mathcal{S}_{OCC}$ and
$\log \proba[lm]{w}$ are negative quantities: by subtracting one to
the other we balance two properties of a good rewriting. $\lambda \in
\left[0,1\right]$ is a parameter to control this balancing.

Another take on this problem is to consider that good lexical
rewritings are good rewritings of rarely occurring words. This
definition is well captured by the conditional probability defined in
\equaref{eq:probcond} and its application to the score
$\mathcal{S}_{LMc}$ defined in \equaref{eq:slmc}. $c$ stands for
“conditional”:

\begin{align}
  \label{eq:probcond}
  \proba{w \maps{D} r \given w \maps{D} \_} & = \frac%
  {\card{w \maps{D} r}}%
  {\card{w \maps{D} \_}} \\
  \label{eq:slmc}
  \mathcal{S}_{LMc}(w \maps{D} r) & = \log \frac%
  {\proba{w \maps{D} r \given w \maps{D} \_}^\lambda}%
  {\proba[LM]{w}^{(1 - \lambda)}}
\end{align}

The scores $\mathcal{S}_{LM}$ and $\mathcal{S}_{LMc}$ already have
nice qualities to model different definitions of good rewritings. But
as they stand, the lengthier the word being rewritten, the better the
score of its rewriting, almost regardless of whether or not the
rewriting is a good lexical rewriting. This happens because the
language model probability of a $(n+1)$-gram is almost always an order
of magnitude lower than the language model probability of a
$n$-gram.

Concretely it means that the rewriting of a bigram such as “\emph{,
  and} $\rightarrow$ \emph{.}”  will come above most lexical
rewritings, such as the rewriting “\emph{exhausted} $\rightarrow$
\emph{tired}”. Taking into account the length of the sequence being
rewritten allows to fix that. The scoring functions
$\mathcal{S}_{LMw}$ and $\mathcal{S}_{LMcw}$ are extensions of
respectively the scoring functions $\mathcal{S}_{LM}$ and
$\mathcal{S}_{LMc}$ and are introduced to average the language model
scores by considering the length of the words considered. $w$ stands
for “weighted”:

\begin{align}
  \label{eq:slmw}
  \mathcal{S}_{LMw}(w \maps{D} r) = & \log \frac%
  {\proba{w \maps{D} r}^\lambda}%
  {\sqrt[\card{w}]{\proba[LM]{w}}^{(1 - \lambda)}} \\
  \label{eq:slmcw}
  \mathcal{S}_{LMcw}(w \maps{D} r) = & \log \frac%
  {\proba{w \maps{D} r \given w \maps{D} \_}^\lambda}%
  {\sqrt[\card{w}]{\proba[LM]{w}}^{(1 - \lambda)}}
\end{align}

The method used in both \equaref{eq:slmw} and \equaref{eq:slmcw} to
weight a language model score by its length $l$ is to take its
$l^{th}$-root. This is justified by the fact that language model
scores follow a power law when subjected to the length of their input
sequences.

Note that while we progressively improved the lexical properties of
the scoring functions over the course of this section, the results of
those functions are not the log of probabilities anymore, which is not
a problem to compare rewritings but might very well be to use those
rewriting scores in a broader framework, for example when performing
the automatic rewriting of a text. We will later on address this
issue.

\section{Fine-grained readability assessment}
\label{sec:fine-grain-read}

Having defined lexical enhancements and a way to score them, we now
have: (1.) a way to detect which parts of a text need to be rewritten;
(2.) a score to estimate the importance of those rewritings.

Combining elements (1.) and (2.) we can define a fine-grained
readability measure of a text as follows:

\begin{itemize}
\item the readability of a part of a sentence is the combination of
  the scores of its possible rewritings;
\item the readability of a sentence is the combination of the
  readability of its parts;
\item the readability of a text is the combination of the readability
  of its sentences.
\end{itemize}

We chose those three levels (sentence parts, sentence, text) because
they are the best fit to the alignments made to produce the lexical
enhancement dictionary.

Formally, we define a Recursive Lexical Readability Measure
$\mathcal{RLRM}$ of a text $T$ using a lexical enhancement dictionary
$\mathcal{D}$ as the combination of the three functions $\theta$,
$\sigma$ and $\pi$—named after the initials of text, sentence and
part:
\[
\mathcal{RLRM}(t) = \theta\Bigg(\Bigg\{ \sigma \left(\left\{ \pi(p
    \maps{D} r) \suchthat \text{$p$ is a part of $s$}\right\}\right)
\,\Bigg|\, \text{$s$ is a sentence in $T$} \Bigg\}\Bigg)
\]
To define a $\mathcal{RLRM}$, one needs to provide the three functions
$\theta$, $\sigma$ and $\pi$. While this is quite a general framework,
at the time of this writing, only the function $\mathcal{RLRM}_{max}$
with $\theta = \text{average}$ and $\sigma = \pi = \max$ has been
investigated. Such a function can be used as a readability measure and
its construction allows for fine-grained annotation of the readability
of a text.

The next section details another possible usage of a lexical
enhancement dictionary by considering it as a basis for a translation
task.

\section{Automatic rewriting of texts}
\label{sec:rewriting}

To rewrite texts automatically, we use weighted finite-state
transducers that consider the scores defined
in \sectionref{sec:lexical-enhancements} as weights to rewrite any
input text. By doing so we consider the rewriting of texts as a
translation task from the language of hardly readable texts to the
language of readable texts.

Two of the main tasks performed while manipulating weighted
transducers are combining weights in a given branch and combining the
weights of two different branches. Those two operations require two
closed binary operations and therefore semi-rings are commonly used to
model the way weights are handled in a weighted transducer
\cite{mohri2004weighted}. However the end-goal of using transducers
being to use the Viterbi algorithm \cite{forney1973viterbi}, we only
need one binary operation (the need to combine branches never occurs
during Viterbi). Therefore we part from the common definition, for
simplicity, and use monoids instead of semi-rings.

We define a weighted finite-state transducer over a monoid
$(\mathbb{K}, \otimes, \mathbb{1})$ (that we will use to combine the
outputs of a score function) as an 8-tuple that follows the common
definition used by \cite{mohri2004weighted}: $T = (\Sigma, \Delta, Q,
I, F, E, \lambda, \rho)$ where:
\begin{itemize}
\item $\Sigma$ is the finite input alphabet of the transducer;
\item $\Delta$ is the finite output alphabet;
\item $Q$ is a finite set of states;
\item $I \subseteq Q$ the set of initial states;
\item $F \subseteq Q$ the set of final states;
\item $E \subseteq Q \times (\Sigma \cup \{\varepsilon\}) \times
  (\Delta \cup \{\varepsilon\}) \times \mathbb{K} \times Q$ a finite
  set of transitions;
\item $\lambda : I \rightarrow \mathbb{K}$ the initial weight function;
\item $\rho : F \rightarrow \mathbb{K}$ the final weight function mapping
  $F$ to $\mathbb{K}$.
\end{itemize}
Such a transducer over a monoid $(\mathbb{K}, \otimes, \mathbb{1})$ is
constructed for a given input text of $n$ tokens noted $t_1 \dots t_n$
and their $n$ parts of speech noted $p_1 \dots p_n$ as follows, where
$\mathcal{T}$ is the set of all tokens in written English,
$\mathcal{P}$ is the set of all parts of speech in English and
$\mathcal{S} : \mathcal{D} \rightarrow \mathbb{K}$ is the score we
use, $\mathbb{K}$ being the set of the monoid:
\begin{align*}
\Sigma & = \mathcal{T} \times \mathcal{P} \\
\Delta & = \mathcal{T} \times \mathcal{P} \\
Q & = \left\{ q_i \suchthat 0 \leq i \leq n \right\} \\
I & = \{ q_0 \} \\
F & = \{ q_n \} \\
E & = \left\{ (q_{i - 1}, w, r, s, q_i) \suchthat w \maps{D} r
    \land w = t_i \land p_w = p_i \land s = \mathcal{S}\left(\ t_i
      \maps{D} r \right) \land 1 \leq i \leq n \right\} \\
  & \quad \, \cup \left\{ (q_{i - 1}, t_i, t_i, \mathbb{1}, q_i)
    \suchthat 1 \leq i \leq n \right\} \\
\lambda & : x \mapsto \mathbb{1} \\
\rho & : x \mapsto \mathbb{1}
\end{align*}

Now, we can define monoids to use with the constructed transducer. We
mainly consider two use-cases, each of them requiring a different
monoid:

\begin{figure}[H]
  \centering
  \begin{enumerate}
  \item from the transducer and an input text, calculate the top $n$
    transductions. This is the classical task of machine translation;
  \item from the transducer and an input text, get an idea of how the
    system rewrites the text at a certain level of confidence.
  \end{enumerate}
  \caption{Use-cases of text rewriting}
\label{fig:use-cases}
\end{figure}

The motivation for the second use-case is maybe not clear at
first. The reason it needs to be separated from use-case (1.) is that,
even for a very long text, the top 100 scores of the rewritings will
likely correspond to 100 different single token modifications while we
would want to see the complete text rewritten at once at a certain
level of confidence. An example will illustrate the key difference
later on in this section.

We define the monoids in \tableref{tab:monoids} for the $S_{OCC}$
score function. The Negative Min monoid allows us to obtain the
threshold effect of use-case (2.) from \figureref{fig:use-cases} while
the Negative Log monoid allows us to compute the precise tops of
use-case (1.). Negative Log is the log equivalent of the $(\left[0,
  1\right], *, 1)$ monoid for probabilities.

\begin{table}[H]
  \centering
  \caption{Interesting monoids for our score functions}
  \begin{tabular}{rccc}
    \toprule
    Semiring & $\mathbb{K}$ & $\otimes$ & $\mathbb{1}$ \\
    \midrule
    Negative Log & $\mathbb{R}^{-} \cup \{-\infty\}$ & $+$ & $0$ \\
    Negative Min & $\mathbb{R}^{-} \cup \{-\infty\}$ & $\min$ & $0$ \\
  \end{tabular}
  \label{tab:monoids}
\end{table}

To illustrate how those monoids work, let us consider an example
transducer for the sentence “I'm exhausted”
(\figureref{fig:transducer-ex}). The convention used to represent the
transducer is that the first line of the transition represents the
symbol read, the second represents the output symbol and the third is
the weight of the transition.

As detailed above, a word can output itself (the black plain
transitions) for free and any other transition (the red dotted
transitions) will be weighted by a score—here the scores are arbitrary
but the end goal is of course to use the scores defined
in \sectionref{sec:lexical-enhancements}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{semirings}
  \caption{An example transducer for the sentence “I'm exhausted”}
  \label{fig:transducer-ex}
\end{figure}

With the monoids defined in \tableref{tab:monoids}, we would obtain
the rewritings shown in \tableref{tab:neglog}. We can clearly observe
the different behaviors of the two monoids in the last row of the
table. While the monoids have similar effects for single
modifications, the desired threshold effect is obtained with Negative
Min and the precise transduction score is retrieved with Negative
Log when considering rewritings with two non-identity transitions.

\begin{table}[H]
  \centering
  \caption{Rewritings of “I'm exhausted”}
  \begin{tabular}[H]{SSl}
    \toprule
    {Negative Log Score} & {Negative Min Score} & {Rewriting} \\
    \midrule
    0 & 0 & I'm exhausted \\
    -0.9 & -0.9 & I'm tired \\
    -3.2 & -3.2 & I am exhausted \\
    -4.1 & -3.2 & I am tired \\
  \end{tabular}
  \label{tab:neglog}
\end{table}

The two monoids are almost usable with the various flavors of
$\mathcal{S}_{LM}$ score functions. But since they are not the log of
probabilities, we don't have the guarantee that for all $w \maps{D}
r$, $\mathcal{S} \left( w \maps{D} r \right) \in \mathbb{R}^{-} \cup
\{-\infty\}$. To fix this problem and be able to use the semi-rings
defined above, we normalize the scores $\mathcal{S}_{LM}$ and
$\mathcal{S}_{LMc}$ into the scores $\mathcal{S}_{LMn},
\mathcal{S}_{LMcn}$ as follows ($n$ standing for normalized):

\[
\mathcal{S}_{n} \left( w \maps{D} r \right) = \mathcal{S} \left( w
  \maps{D} r \right) + \argmin_{w'} \left( \log \proba[LM]{w'}^{(1 - \lambda)} \right)
\]

And the scores $\mathcal{S}_{LMw}$ and $\mathcal{S}_{LMcw}$ into the
scores $\mathcal{S}_{LMwn}, \mathcal{S}_{LMcwn}$ as follows:

\[
\mathcal{S}_{n} \left( w \maps{D} r \right) = \mathcal{S} \left( w
  \maps{D} r \right) + \argmin_{w'} \left( \log
  \sqrt[\card{w'}]{\proba[LM]{w'}^{(1 - \lambda)}} \right)
\]

The normalized functions can be used with the monoids we defined for
$\mathcal{S}_{OCC}$. Note that if we had used semi-rings, this
normalization would be incorrect because we do not obtain the log of a
probability distribution after this normalization process. However,
while it is a problem to combine the weights of different branches, it
is not a problem to combine weights in a branch.

It is now possible to address the two use-cases mentioned in
\figureref{fig:use-cases} by using the Viterbi algorithm
\cite{forney1973viterbi} with Negative Log as monoid for the first
use-case and Negative MaxMin for the second.

The next section details \textsc{Readability Lab}, a framework built
to analyze both the lexical enhancements and the text rewritings
introduced in \sectionref{sec:lexical-enhancements}
and \sectionref{sec:rewriting} in an intuitive and efficient manner.

\section{\textsc{Readability Lab}, a fine-grained readability analysis
  framework}
\label{sec:framework}

Considering that automatic evaluation of readability rewriting and
lexical enhancement is a very hard task, producing a high quality
testing environment to be able to quickly assess manually the quality
of an approach is valuable. It was also the occasion to expose those
results to other programs by creating a server that could easily be
used by from other tool-chain.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{ui}
  \caption{\textsc{Readability Lab} interface}
  \label{fig:ui}
\end{figure}

\textsc{uimaFIT}, \textsc{Play
  Framework}\footnote{\url{http://www.playframework.com/}},
\textsc{React}\footnote{\url{https://facebook.github.io/react/}} and
\textsc{Heroku}\footnote{\url{https://www.heroku.com/}} were used to
create a web client available at
\url{http://readability.crydee.eu/}. It satisfies the constraints
exposed above. \figureref{fig:ui} shows a screenshot of the interface.

Among the theoretical obstacles faced during the creation of this
demo, the most interesting one is certainly the algorithm used to
detect matches in a text. Indeed, with more than \numprint{30000}
patterns to test, response times easily grow slow. The Aho-Corasick
algorithm was used to address this issue \cite{aho1975efficient}. The
principle of this algorithm is to build an automaton to match a wide
range of patterns that is afterwards usable on any incoming text. The
only unusual aspect in our use of this algorithm is the alphabet we
use: in our case it is the alphabet of pairs of tokens and their part
of speech instead of the classical English alphabet.

This two-step process—slow automaton building and fast searches
afterwards—matches perfectly the structure of a web application where
the starting phase is usually slow, allowing lengthy initialization
processes after which incoming requests have to be handled
promptly. Since no existing implementation that we knew of had
abstracted away the alphabet used, a new implementation has been
proposed that addresses this issue and allows to use arbitrary Java
classes as alphabet (here our pairs of token and their part of
speech). It is freely
available\footnote{\url{https://github.com/m09/aho-corasick}}.

The source of \textsc{Readability Lab} is—yet again—available on
\textsc{Github}\footnote{\url{https://github.com/m09/readability}}.

\chapter{Evaluation \& Discussion}
\label{cha:discussion}

To evaluate and discuss the work proposed in \chapterref{cha:work},
several approaches have been used in the litterature. The main
possibilities are:

\begin{enumerate}
\item the use of machine translation measures, such as BLEU
  \cite{papineni2002bleu}, ROUGE \cite{lin2004rouge} or NIST
  \cite{doddington2002automatic};
\item using readability formulas as scores, including Dale−Chall and
  Flesch Reading Ease;
\item evaluating by comparing agreement among several lexical
  enhancement sources. In their 2010 paper, \cite{yatskar2010sake}
  proposed an evaluation based on a list of interesting rewritings
  built by a \sew{} contributor. However the list does not seem to be
  available anymore;
\item manual evaluation.
\end{enumerate}

Most of these possibilities are flawed in some way: BLEU, ROUGE and
NIST have been built to evaluate a translation from one language to
another. For example, when scoring an English to French translation,
the starting score is $0$. When scoring a not-readable English to
readable English translation, the starting score (when the case the
translation doesn't actually do anything) could already be extremely
high, consider the gold translation:

“I went to the park and enjoyed the fantastic weather.” $\rightarrow$
“I went to the park and enjoyed the great weather.”

Compared to it, a NO-OP translation will have a unigram precision of
$\,\sfrac{9}{10}$, a bigram precision of $\,\sfrac{7}{9}$, etc. The
resulting BLEU, ROUGE and NIST scores will be very high.

Readability formulas are also flawed, as detailed
in \chapterref{cha:sota}. A better alternative is to use the most
recent machine learning techniques as baselines, at the price of
implementation troubles.

Finally manual evaluation is flawed in the sense that it doesn't cover
a huge amount of data and has the usual problems of
annotation—subjectivity and consistency probably being the main
ones.

At the time of this writing, the automatic evaluation is not yet in a
presentable form and we settle for manual evaluation and discussion.

The main point to evaluate in this work is the efficiency of the five
different scoring functions. They are analyzed below. To determine the
problems and advantages of the scoring functions, an empirical study
was led with a range of news articles. This informal insight still has
to be backed up by further experiments at the time of this
writing. All experiments were done with the $\lambda$ parameter fixed
at $0.5$.

\paragraph*{$\mathcal{S}_{OCC}$ function}
\label{par:occ}

Counting occurrences has an important bias towards non-content
words—they are by far the most frequent in most languages, including
English. They occupy the vast majority of the top suggestions
spots. This is an important problem for a lexical approach. Indeed,
non-content words mostly serve syntactical purposes. Trying to improve
on the syntactic structure without further consideration for
grammatical context is bound to fail. Despite these important
problems, when lexical matches are found in the top rewritings they
tend to be of good quality, as shown in \tableref{tab:tops-socc}.

\begin{table}[H]
  \centering
  \caption{$\mathcal{S}_{OCC}$ top 5 rewritings}
  \begin{tabularx}{\textwidth}{SXX}
    \toprule
    {Score} & Original & Rewriting \\
    \midrule
    -4.99 & , & . \\
    -5.10 & external links & other websites \\
    -5.26 & discovered & found \\
    -5.32 & , and & . \\
    -5.62 & ; & . \\
  \end{tabularx}
  \label{tab:tops-socc}
\end{table}

\paragraph*{$\mathcal{S}_{LMn}$ and $\mathcal{S}_{LMcn}$ functions}
\label{par:lmn}

Balancing the $S_{OCC}$ score with a language model is not enough, as
quickly appears from glancing at the top rewritings shown in
\tableref{tab:tops-slmn}. All the best rewritings are of the maximum
length ($5$ tokens) and are mostly noise. Indeed, the noisier some
sequence is, the lower its language model score is and the higher its
$\mathcal{S}_{LMn}$ and $\mathcal{S}_{LMcn}$ scores are.

\begin{table}[H]
  \centering
  \caption{$\mathcal{S}_{LMn}$ and $\mathcal{S}_{LMcn}$ top 5 rewritings}
  \begin{tabularx}{\textwidth}{SS>{\hsize=1.2\hsize}X>{\hsize=0.8\hsize}X}
    \toprule
    {$\mathcal{S}_{LMn}$} & {$\mathcal{S}_{LMcn}$} & Original & Rewriting \\
    \midrule
    -10.48 & 0.00  & slit-like vertical external urethral orifice & urethra \\
    -11.87 & -1.38 & cover throughout entire catchments, & unchanged. \\
    -12.52 & -2.03 & :cognitive biases category:group processes & :psicology \\
    -13.21 & -2.72 & heading a mclaren one-two with & and his \\
    -13.72 & -3.23 & recieve a hard sting from & were stung by \\
  \end{tabularx}
  \label{tab:tops-slmn}
\end{table}

\newpage

\paragraph*{$\mathcal{S}_{LMwn}$ function}
\label{par:lmn}

Once averaged by the sequence length, the language model balancing
yields better results as shown in \tableref{tab:tops-slmwn}. While the
results seem very good in this top 5, it appears empirically that the
scoring is inconsistent: many poor rewritings have good relative
scores compared to interesting ones. Further study would be needed to
see if these inconsistencies can be addressed by setting the $\lambda$
parameter to a more efficient value.
\begin{table}[H]
  \centering
  \caption{$\mathcal{S}_{LMwn}$ top 5 rewritings}
  \begin{tabularx}{\textwidth}{SXX}
    \toprule
    {Score} & Original   & Rewriting \\
    \midrule
    -9.75   & obtain     & get       \\
    -9.79   & discovered & found     \\
    -10.10  & n't        & not       \\
    -10.17  & indicate   & show      \\
    -10.34  & allow      & let       \\
  \end{tabularx}
  \label{tab:tops-slmwn}
\end{table}

\paragraph*{$\mathcal{S}_{LMcwn}$ function}
\label{par:lmn}

Finally, by using the conditional probability of a rewriting occurring
when trying to rewrite a given word instead of the straight
probability of a rewriting occurring, the results obtained typically
seem of lower quality than the results of the $\mathcal{S}_{LMwn}$
function, as shown in \tableref{tab:tops-slmcwn}. It is however
interesting to discuss which use-case we are evaluating:

\begin{itemize}
\item if we are evaluating the best rewritings without any context,
  then this function is worse than $\mathcal{S}_{LMwn}$;
\item if we are trying to simplify a text, then the noisy entries such
  as “"he” and “exaggerated” do not matter so much, since they are
  unlikely to appear in the words to rewrite.
\end{itemize}

We can conclude that this function is very dependent on the corpus
quality. For the corpus we gathered, in practice, it is the function
that yields the best results in our experience.

As for the rest of this evaluation, further study is necessary to
confirm or infirm this empirical conclusion—it is not backed by enough
data.

\begin{table}[H]
  \centering
  \caption{$\mathcal{S}_{LMcwn}$ top 5 rewritings}
  \begin{tabularx}{\textwidth}{SXX}
    \toprule
    {Score} & Original & Rewriting \\
    \midrule
    0.00 & "he & that while giotto \\
    -0.51 & exaggerated & said that \\
    -0.57 & stray & wander \\
    -0.60 & virtuous & good \\
    -0.64 & discoloration & colour \\
  \end{tabularx}
  \label{tab:tops-slmcwn}
\end{table}

\chapter{Future work}
\label{cha:future-work}

There are many possibilities for future work. Among the most important
ones are:

\begin{enumerate}
\item quantify the impact of the amount of data on the approaches
  proposed. Around \numprint{30000} lexical enhancements, while a good
  start, is certainly a number that could grow with great effects for
  our statistical approaches;
\item better handle grammaticality in the lexical rewriting context by
  using syntactic parsing information to constrain rewritings is a
  promising possibility;
\item investigate syntactic readability issues in the fine-grained
  analysis context, in particular by going from a string to string
  transduction process to a tree to tree transduction process,
  following related work in text rewriting;
\item integrate gaze information in the process of estimating the
  readability of an area of a text. It should combine especially well
  with lexical estimations, because both can be anchored precisely in
  a text. Conversely, it might be more challenging to improve a
  syntactic approach with gaze information.
\item conduct a thorough evaluation, including automatic evaluation
  with readability measures, lexical improvement lists comparison and
  manual evaluation. An idea to evaluate manually the quality of the
  rewritings proposed is to submit two rewritings, one of our own and
  one of a similar approach to a human judge together with the
  original text being rewritten and ask the judge to compare the two
  rewritings.
\end{enumerate}

During the month left of this internship, we plan on focusing on tree
transducing (3.) and a better evaluation (5.).

\chapter{Conclusion}

In this work we studied how to use classical machine translation and
readability techniques to obtain fine-grained measurements of the
readability of a text.

\bibliographystyle{apalike2}
\bibliography{report.bib}

\appendix

\chapter{Open source contributions}
\label{cha:oss-contribs}

During this internship I had the opportunity to contribute to the open
source natural language processing community. What follows is a list
of those contributions. When no link is provided, the software is part
of the \textsc{Readability Lab} repository already linked several
times in this report.

\section{Enhancements of existing software}
\label{sec:enhancements}

\begin{itemize}
\item
  \href{https://bugs.eclipse.org/bugs/show_bug.cgi?id=433163}{Wikitext
    parser bug filing}
\item \href{https://issues.apache.org/jira/browse/OPENNLP-676}{OpenNLP
    POSTagger bug filing with patch}
\item \href{https://issues.apache.org/jira/browse/UIMA-3913}{uimaFIT
    enhancement proposal}
\end{itemize}

\section{\textsc{uimaFIT} components}
\label{sec:uimafit-components}

\begin{itemize}
\item sentence alignment analysis engine, using Myers' algorithm
\item word alignment analysis engine, using yet again Myers' algorithm
\item \href{https://berkeleylm.googlecode.com/}{BerkeleyLM} uimaFIT
  wrapper
\item readability formulas uimaFIT calculator
\end{itemize}

\section{Miscellaneous}
\label{sec:misc-software}
\begin{itemize}
\item Wikimedia PostgreSQL importer
\item Generic implementation of Aho-Corasick
\end{itemize}

\end{document}
%%% Local Variables: 
%%% coding: utf-8
%%% mode: latex
%%% TeX-engine: xetex
%%% End: 
